#!/usr/bin/env python3
"""
ç±»æ–¹æ³•æ˜ å°„å·¥å…·
ç”¨äºåˆ†æé¡¹ç›®ç›®å½•ä¸­çš„ç±»å’Œå‡½æ•°ç»“æ„ï¼Œç”Ÿæˆclass_method_map.md
æ”¯æŒå¤šè¯­è¨€é¡¹ç›®åˆ†æå’Œå¢å¼ºåŠŸèƒ½
"""
import ast
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
import re
from datetime import datetime


class ClassMethodMapper:
    """ç±»æ–¹æ³•æ˜ å°„å™¨ï¼Œç”¨äºæå–é¡¹ç›®ä¸­çš„ç±»å’Œå‡½æ•°ç»“æ„"""
    
    def __init__(self, project_path: Path):
        self.project_path = project_path
        self.class_map: Dict[str, Dict] = {}
        self.function_map: Dict[str, Dict] = {}
        self.imports_map: Dict[str, List] = {}
        self.file_structure: Dict[str, List] = {}
        
    def analyze_project(self) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        print(f"ğŸ” å¼€å§‹åˆ†æé¡¹ç›®ç»“æ„: {self.project_path}")
        
        # æ¸…ç©ºä¹‹å‰çš„åˆ†æç»“æœ
        self.class_map.clear()
        self.function_map.clear()
        self.imports_map.clear()
        self.file_structure.clear()
        
        # æŸ¥æ‰¾æ‰€æœ‰Pythonæ–‡ä»¶
        python_files = list(self.project_path.rglob("*.py"))
        
        print(f"ğŸ“ æ‰¾åˆ° {len(python_files)} ä¸ªPythonæ–‡ä»¶")
        
        analyzed_count = 0
        for file_path in python_files:
            # è·³è¿‡è™šæ‹Ÿç¯å¢ƒç›®å½•å’Œç¼“å­˜ç›®å½•
            if any(skip in str(file_path) for skip in ['.venv', '__pycache__', '.git', '.aacode']):
                continue
                
            try:
                self._analyze_file(file_path)
                analyzed_count += 1
            except Exception as e:
                print(f"âš ï¸  åˆ†ææ–‡ä»¶ {file_path.relative_to(self.project_path)} æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… æˆåŠŸåˆ†æ {analyzed_count} ä¸ªæ–‡ä»¶")
        return self._generate_summary()
    
    def _analyze_file(self, file_path: Path):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            content = file_path.read_text(encoding='utf-8')
            tree = ast.parse(content)
            
            relative_path = file_path.relative_to(self.project_path)
            file_key = str(relative_path)
            
            # è®°å½•æ–‡ä»¶ç»“æ„
            self.file_structure[file_key] = []
            
            # åˆ†æå¯¼å…¥
            imports = self._extract_imports(tree)
            if imports:
                self.imports_map[file_key] = imports
            
            # åˆ†æç±»å’Œå‡½æ•°
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_info = self._analyze_class(node, file_key, content)
                    self.file_structure[file_key].append({
                        'type': 'class',
                        'name': class_info['name'],
                        'line': class_info['line']
                    })
                elif isinstance(node, ast.FunctionDef):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç±»æ–¹æ³•
                    parent = self._get_parent(node, tree)
                    if not isinstance(parent, ast.ClassDef):
                        func_info = self._analyze_function(node, file_key, content)
                        self.file_structure[file_key].append({
                            'type': 'function',
                            'name': func_info['name'],
                            'line': func_info['line']
                        })
                    
        except SyntaxError as e:
            print(f"âš ï¸  æ–‡ä»¶ {file_path.relative_to(self.project_path)} è¯­æ³•é”™è¯¯: {e}")
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                content = file_path.read_text(encoding='gbk')
                self._analyze_file_content(content, file_path)
            except:
                print(f"âš ï¸  æ— æ³•è§£ææ–‡ä»¶ {file_path.relative_to(self.project_path)} çš„ç¼–ç ")
    
    def _analyze_file_content(self, content: str, file_path: Path):
        """åˆ†ææ–‡ä»¶å†…å®¹"""
        tree = ast.parse(content)
        relative_path = file_path.relative_to(self.project_path)
        file_key = str(relative_path)
        
        # åˆ†æç±»å’Œå‡½æ•°
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                self._analyze_class(node, file_key, content)
            elif isinstance(node, ast.FunctionDef):
                parent = self._get_parent(node, tree)
                if not isinstance(parent, ast.ClassDef):
                    self._analyze_function(node, file_key, content)
    
    def _extract_imports(self, tree: ast.AST) -> List[Dict]:
        """æå–å¯¼å…¥è¯­å¥"""
        imports = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append({
                        'type': 'import',
                        'module': alias.name,
                        'alias': alias.asname,
                        'lineno': node.lineno
                    })
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ''
                for alias in node.names:
                    imports.append({
                        'type': 'from_import',
                        'module': module,
                        'name': alias.name,
                        'alias': alias.asname,
                        'lineno': node.lineno
                    })
        
        return imports
    
    def _analyze_class(self, node: ast.ClassDef, file_key: str, content: str) -> Dict:
        """åˆ†æç±»å®šä¹‰"""
        class_name = node.name
        full_class_name = f"{file_key}:{class_name}"
        
        # æå–åŸºç±»
        bases = []
        for base in node.bases:
            if isinstance(base, ast.Name):
                bases.append(base.id)
            elif isinstance(base, ast.Attribute):
                bases.append(self._get_attr_name(base))
        
        # æå–ç±»æ–‡æ¡£å­—ç¬¦ä¸²
        docstring = ast.get_docstring(node)
        
        # æå–æ–¹æ³•
        methods = []
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                method_info = self._extract_method_info(item, content)
                methods.append(method_info)
        
        # æå–ç±»è£…é¥°å™¨
        decorators = []
        for decorator in node.decorator_list:
            decorators.append(self._get_decorator_name(decorator))
        
        # æå–ç±»å±æ€§
        attributes = []
        for item in node.body:
            if isinstance(item, ast.Assign):
                for target in item.targets:
                    if isinstance(target, ast.Name):
                        attributes.append({
                            'name': target.id,
                            'line': item.lineno
                        })
        
        class_info = {
            'name': class_name,
            'file': file_key,
            'line': node.lineno,
            'bases': bases,
            'docstring': docstring or '',
            'methods': methods,
            'attributes': attributes,
            'decorators': decorators,
            'full_name': full_class_name
        }
        
        self.class_map[full_class_name] = class_info
        return class_info
    
    def _analyze_function(self, node: ast.FunctionDef, file_key: str, content: str) -> Dict:
        """åˆ†æå‡½æ•°å®šä¹‰"""
        function_name = node.name
        full_function_name = f"{file_key}:{function_name}"
        
        function_info = self._extract_function_info(node, content)
        function_info.update({
            'name': function_name,
            'file': file_key,
            'line': node.lineno,
            'full_name': full_function_name
        })
        
        self.function_map[full_function_name] = function_info
        return function_info
    
    def _extract_method_info(self, node: ast.FunctionDef, content: str) -> Dict:
        """æå–æ–¹æ³•ä¿¡æ¯"""
        info = self._extract_function_info(node, content)
        info['name'] = node.name
        return info
    
    def _extract_function_info(self, node: ast.FunctionDef, content: str) -> Dict:
        """æå–å‡½æ•°/æ–¹æ³•ä¿¡æ¯"""
        # æå–å‚æ•°
        args = []
        
        # ä½ç½®å‚æ•°
        for arg in node.args.args:
            args.append(arg.arg)
        
        # æå–é»˜è®¤å‚æ•°
        defaults = []
        for default in node.args.defaults:
            try:
                defaults.append(ast.unparse(default) if hasattr(ast, 'unparse') else str(default))
            except:
                defaults.append(str(default))
        
        # æå–æ–‡æ¡£å­—ç¬¦ä¸²
        docstring = ast.get_docstring(node)
        
        # æå–è¿”å›ç±»å‹æ³¨è§£
        returns = None
        if node.returns:
            try:
                returns = ast.unparse(node.returns) if hasattr(ast, 'unparse') else str(node.returns)
            except:
                returns = str(node.returns)
        
        # æå–è£…é¥°å™¨
        decorators = []
        for decorator in node.decorator_list:
            decorators.append(self._get_decorator_name(decorator))
        
        return {
            'args': args,
            'defaults': defaults,
            'docstring': docstring or '',
            'returns': returns,
            'decorators': decorators,
            'is_async': isinstance(node, ast.AsyncFunctionDef)
        }
    
    def _get_attr_name(self, node: ast.Attribute) -> str:
        """è·å–å±æ€§åç§°"""
        if isinstance(node.value, ast.Name):
            return f"{node.value.id}.{node.attr}"
        elif isinstance(node.value, ast.Attribute):
            return f"{self._get_attr_name(node.value)}.{node.attr}"
        else:
            return node.attr
    
    def _get_decorator_name(self, node: ast.AST) -> str:
        """è·å–è£…é¥°å™¨åç§°"""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return self._get_attr_name(node)
        elif isinstance(node, ast.Call):
            if isinstance(node.func, ast.Name):
                return node.func.id
            elif isinstance(node.func, ast.Attribute):
                return self._get_attr_name(node.func)
        return str(node)
    
    def _get_parent(self, node: ast.AST, tree: ast.AST) -> Optional[ast.AST]:
        """è·å–èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹"""
        # ç®€å•çš„çˆ¶èŠ‚ç‚¹æŸ¥æ‰¾
        for parent in ast.walk(tree):
            for child in ast.iter_child_nodes(parent):
                if child is node:
                    return parent
        return None
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        return {
            'project_path': str(self.project_path),
            'class_count': len(self.class_map),
            'function_count': len(self.function_map),
            'file_count': len(self.file_structure),
            'classes': list(self.class_map.keys()),
            'functions': list(self.function_map.keys()),
            'file_structure': self.file_structure
        }
    
    def generate_class_method_map(self) -> str:
        """ç”Ÿæˆç±»æ–¹æ³•æ˜ å°„çš„Markdownæ–‡æ¡£"""
        lines = []
        
        lines.append("# é¡¹ç›®ç±»æ–¹æ³•æ˜ å°„")
        lines.append("")
        lines.append(f"é¡¹ç›®è·¯å¾„: `{self.project_path}`")
        lines.append(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"ç±»æ•°é‡: {len(self.class_map)}")
        lines.append(f"å‡½æ•°æ•°é‡: {len(self.function_map)}")
        lines.append(f"æ–‡ä»¶æ•°é‡: {len(self.file_structure)}")
        lines.append("")
        
        # æ–‡ä»¶ç»“æ„æ¦‚è§ˆ
        lines.append("## æ–‡ä»¶ç»“æ„æ¦‚è§ˆ")
        lines.append("")
        
        for file_path, items in sorted(self.file_structure.items()):
            class_count = sum(1 for item in items if item['type'] == 'class')
            function_count = sum(1 for item in items if item['type'] == 'function')
            
            lines.append(f"- `{file_path}`")
            lines.append(f"  - ç±»: {class_count} ä¸ª")
            lines.append(f"  - å‡½æ•°: {function_count} ä¸ª")
        lines.append("")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºç±»
        files_with_classes = {}
        for class_name, class_info in self.class_map.items():
            file_path = class_info['file']
            if file_path not in files_with_classes:
                files_with_classes[file_path] = []
            files_with_classes[file_path].append(class_info)
        
        if files_with_classes:
            lines.append("## ç±»å®šä¹‰è¯¦æƒ…")
            lines.append("")
            
            for file_path, classes in sorted(files_with_classes.items()):
                lines.append(f"### æ–‡ä»¶: `{file_path}`")
                lines.append("")
                
                for class_info in sorted(classes, key=lambda x: x['line']):
                    lines.append(f"#### ç±»: `{class_info['name']}`")
                    lines.append("")
                    
                    # åŸºæœ¬ä¿¡æ¯
                    lines.append(f"- **ä½ç½®**: ç¬¬ {class_info['line']} è¡Œ")
                    
                    if class_info['bases']:
                        bases_str = ', '.join([f"`{base}`" for base in class_info['bases']])
                        lines.append(f"- **ç»§æ‰¿è‡ª**: {bases_str}")
                    
                    if class_info['decorators']:
                        decorators_str = ', '.join([f"`{dec}`" for dec in class_info['decorators']])
                        lines.append(f"- **è£…é¥°å™¨**: {decorators_str}")
                    
                    if class_info['docstring']:
                        doc_preview = class_info['docstring'].split('\n')[0][:100]
                        lines.append(f"- **æ–‡æ¡£**: {doc_preview}...")
                    
                    # å±æ€§
                    if class_info['attributes']:
                        lines.append("- **å±æ€§**:")
                        for attr in class_info['attributes']:
                            lines.append(f"  - `{attr['name']}` (ç¬¬ {attr['line']} è¡Œ)")
                    
                    # æ–¹æ³•
                    if class_info['methods']:
                        lines.append("- **æ–¹æ³•**:")
                        for method in class_info['methods']:
                            args_str = ', '.join(method['args'])
                            method_desc = f"`{method['name']}({args_str})`"
                            if method['is_async']:
                                method_desc = f"`async {method_desc}`"
                            if method['decorators']:
                                decorators = ', '.join([f"@{dec}" for dec in method['decorators']])
                                method_desc = f"{decorators} {method_desc}"
                            lines.append(f"  - {method_desc}")
                    
                    lines.append("")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºå‡½æ•°
        files_with_functions = {}
        for func_name, func_info in self.function_map.items():
            file_path = func_info['file']
            if file_path not in files_with_functions:
                files_with_functions[file_path] = []
            files_with_functions[file_path].append(func_info)
        
        if files_with_functions:
            lines.append("## å‡½æ•°å®šä¹‰è¯¦æƒ…")
            lines.append("")
            
            for file_path, functions in sorted(files_with_functions.items()):
                lines.append(f"### æ–‡ä»¶: `{file_path}`")
                lines.append("")
                
                for func_info in sorted(functions, key=lambda x: x['line']):
                    lines.append(f"#### å‡½æ•°: `{func_info['name']}`")
                    lines.append("")
                    
                    # åŸºæœ¬ä¿¡æ¯
                    lines.append(f"- **ä½ç½®**: ç¬¬ {func_info['line']} è¡Œ")
                    
                    args_str = ', '.join(func_info['args'])
                    lines.append(f"- **å‚æ•°**: `({args_str})`")
                    
                    if func_info['returns']:
                        lines.append(f"- **è¿”å›ç±»å‹**: `{func_info['returns']}`")
                    
                    if func_info['decorators']:
                        decorators_str = ', '.join([f"`{dec}`" for dec in func_info['decorators']])
                        lines.append(f"- **è£…é¥°å™¨**: {decorators_str}")
                    
                    if func_info['is_async']:
                        lines.append("- **ç±»å‹**: `async` å‡½æ•°")
                    
                    if func_info['docstring']:
                        doc_preview = func_info['docstring'].split('\n')[0][:100]
                        lines.append(f"- **æ–‡æ¡£**: {doc_preview}...")
                    
                    lines.append("")
        
        # å¯¼å…¥å…³ç³»
        if self.imports_map:
            lines.append("## å¯¼å…¥å…³ç³»")
            lines.append("")
            
            for file_path, imports in sorted(self.imports_map.items()):
                if imports:
                    lines.append(f"### `{file_path}`")
                    lines.append("")
                    
                    for imp in imports:
                        if imp['type'] == 'import':
                            if imp['alias']:
                                lines.append(f"- `import {imp['module']} as {imp['alias']}`")
                            else:
                                lines.append(f"- `import {imp['module']}`")
                        else:  # from_import
                            if imp['alias']:
                                lines.append(f"- `from {imp['module']} import {imp['name']} as {imp['alias']}`")
                            else:
                                lines.append(f"- `from {imp['module']} import {imp['name']}`")
                    
                    lines.append("")
        
        return '\n'.join(lines)
    
    def save_class_method_map(self, output_file: str = "class_method_map.md") -> Path:
        """ä¿å­˜ç±»æ–¹æ³•æ˜ å°„åˆ°æ–‡ä»¶"""
        content = self.generate_class_method_map()
        output_path = self.project_path / output_file
        output_path.write_text(content, encoding='utf-8')
        print(f"ğŸ“ ç±»æ–¹æ³•æ˜ å°„å·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    # å…¼å®¹æ€§æ–¹æ³•
    def save_enhanced_map(self, output_file: str = "enhanced_project_map.md") -> Path:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šä¿å­˜å¢å¼ºç‰ˆæ˜ å°„ï¼ˆåŸºç¡€ç‰ˆä¸æ”¯æŒï¼‰"""
        raise AttributeError("åŸºç¡€ç‰ˆç±»æ–¹æ³•æ˜ å°„å™¨ä¸æ”¯æŒå¢å¼ºç‰ˆæ˜ å°„")
    
    def get_language_summary(self) -> str:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šè·å–è¯­è¨€æ‘˜è¦ï¼ˆåŸºç¡€ç‰ˆä¸æ”¯æŒï¼‰"""
        raise AttributeError("åŸºç¡€ç‰ˆç±»æ–¹æ³•æ˜ å°„å™¨ä¸æ”¯æŒè¯­è¨€æ‘˜è¦")
    
    def update_analysis(self, changed_files: Optional[List[Path]] = None) -> bool:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šæ›´æ–°åˆ†æï¼ˆåŸºç¡€ç‰ˆä¸æ”¯æŒï¼‰"""
        raise AttributeError("åŸºç¡€ç‰ˆç±»æ–¹æ³•æ˜ å°„å™¨ä¸æ”¯æŒupdate_analysisæ–¹æ³•")
    
    def update_class_method_map(self, changed_files: Optional[List[Path]] = None) -> bool:
        """
        æ›´æ–°ç±»æ–¹æ³•æ˜ å°„
        Args:
            changed_files: å‘ç”Ÿå˜åŒ–çš„æ–‡ä»¶åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™é‡æ–°åˆ†ææ•´ä¸ªé¡¹ç›®
        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        if changed_files is None:
            # é‡æ–°åˆ†ææ•´ä¸ªé¡¹ç›®
            self.analyze_project()
        else:
            # åªæ›´æ–°å˜åŒ–çš„æ–‡ä»¶
            for file_path in changed_files:
                try:
                    # ä»æ˜ å°„ä¸­ç§»é™¤è¯¥æ–‡ä»¶çš„æ—§è®°å½•
                    self._remove_file_from_maps(file_path)
                    # é‡æ–°åˆ†æè¯¥æ–‡ä»¶
                    self._analyze_file(file_path)
                except Exception as e:
                    print(f"âš ï¸  æ›´æ–°æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        
        # ä¿å­˜æ›´æ–°åçš„æ˜ å°„
        self.save_class_method_map()
        return True
    
    def _remove_file_from_maps(self, file_path: Path):
        """ä»æ˜ å°„ä¸­ç§»é™¤æ–‡ä»¶è®°å½•"""
        relative_path = str(file_path.relative_to(self.project_path))
        
        # ä»class_mapä¸­ç§»é™¤
        keys_to_remove = []
        for key in self.class_map.keys():
            if key.startswith(f"{relative_path}:"):
                keys_to_remove.append(key)
        for key in keys_to_remove:
            del self.class_map[key]
        
        # ä»function_mapä¸­ç§»é™¤
        keys_to_remove = []
        for key in self.function_map.keys():
            if key.startswith(f"{relative_path}:"):
                keys_to_remove.append(key)
        for key in keys_to_remove:
            del self.function_map[key]
        
        # ä»imports_mapä¸­ç§»é™¤
        if relative_path in self.imports_map:
            del self.imports_map[relative_path]
        
        # ä»file_structureä¸­ç§»é™¤
        if relative_path in self.file_structure:
            del self.file_structure[relative_path]


def analyze_and_generate_map(project_path: str) -> Path:
    """åˆ†æé¡¹ç›®å¹¶ç”Ÿæˆç±»æ–¹æ³•æ˜ å°„çš„ä¾¿æ·å‡½æ•°"""
    mapper = ClassMethodMapper(Path(project_path))
    mapper.analyze_project()
    return mapper.save_class_method_map()


# ============================================================================
# å¢å¼ºåŠŸèƒ½ï¼šå¤šè¯­è¨€é¡¹ç›®åˆ†æ
# ============================================================================

class MultiLangAnalyzer:
    """å¤šè¯­è¨€ä»£ç åˆ†æå™¨ï¼ˆä»multi_lang_analyzer.pyåˆå¹¶è€Œæ¥ï¼‰"""
    
    # è¯­è¨€æ–‡ä»¶æ‰©å±•åæ˜ å°„
    LANGUAGE_EXTENSIONS = {
        'python': ['.py'],
        'javascript': ['.js', '.jsx'],
        'typescript': ['.ts', '.tsx'],
        'java': ['.java'],
        'go': ['.go'],
        'rust': ['.rs'],
        'c': ['.c', '.h'],
        'cpp': ['.cpp', '.cc', '.cxx', '.hpp', '.hh', '.hxx'],
        'csharp': ['.cs'],
        'ruby': ['.rb'],
        'php': ['.php'],
        'swift': ['.swift'],
        'kotlin': ['.kt', '.kts'],
        'scala': ['.scala'],
        'html': ['.html', '.htm'],
        'css': ['.css'],
        'sql': ['.sql'],
        'shell': ['.sh', '.bash'],
        'yaml': ['.yaml', '.yml'],
        'json': ['.json'],
        'markdown': ['.md', '.markdown'],
        'text': ['.txt'],
    }
    
    def __init__(self, project_path: Path):
        self.project_path = project_path
        self.language_stats: Dict[str, Dict] = {}
        self.file_structure: Dict[str, Dict] = {}
        self.code_entities: Dict[str, List] = {}
        
    def analyze_project(self) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        print(f"ğŸ” å¼€å§‹åˆ†æå¤šè¯­è¨€é¡¹ç›®ç»“æ„: {self.project_path}")
        
        # æ¸…ç©ºä¹‹å‰çš„åˆ†æç»“æœ
        self.language_stats.clear()
        self.file_structure.clear()
        self.code_entities.clear()
        
        # åˆå§‹åŒ–è¯­è¨€ç»Ÿè®¡
        for lang in self.LANGUAGE_EXTENSIONS.keys():
            self.language_stats[lang] = {
                'file_count': 0,
                'total_lines': 0,
                'code_lines': 0,
                'comment_lines': 0,
                'blank_lines': 0,
            }
        
        # åˆ†ææ‰€æœ‰æ–‡ä»¶
        total_files = 0
        analyzed_files = 0
        
        for file_path in self.project_path.rglob("*"):
            if file_path.is_file():
                total_files += 1
                # è·³è¿‡è™šæ‹Ÿç¯å¢ƒç›®å½•å’Œç¼“å­˜ç›®å½•
                if any(skip in str(file_path) for skip in ['.venv', '__pycache__', '.git', '.aacode', 'node_modules', 'target', 'build', 'dist']):
                    continue
                    
                try:
                    lang = self._detect_language(file_path)
                    if lang:
                        self._analyze_file(file_path, lang)
                        analyzed_files += 1
                except Exception as e:
                    print(f"âš ï¸  åˆ†ææ–‡ä»¶ {file_path.relative_to(self.project_path)} æ—¶å‡ºé”™: {e}")
        
        print(f"ğŸ“ æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶ï¼ŒæˆåŠŸåˆ†æ {analyzed_files} ä¸ªä»£ç æ–‡ä»¶")
        return self._generate_summary()
    
    def _detect_language(self, file_path: Path) -> Optional[str]:
        """æ£€æµ‹æ–‡ä»¶è¯­è¨€"""
        ext = file_path.suffix.lower()
        
        for lang, extensions in self.LANGUAGE_EXTENSIONS.items():
            if ext in extensions:
                return lang
        
        return None
    
    def _analyze_file(self, file_path: Path, lang: str):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            relative_path = str(file_path.relative_to(self.project_path))
            
            # æ›´æ–°è¯­è¨€ç»Ÿè®¡
            stats = self._count_file_stats(content)
            self.language_stats[lang]['file_count'] += 1
            self.language_stats[lang]['total_lines'] += stats['total_lines']
            self.language_stats[lang]['code_lines'] += stats['code_lines']
            self.language_stats[lang]['comment_lines'] += stats['comment_lines']
            self.language_stats[lang]['blank_lines'] += stats['blank_lines']
            
            # è§£æä»£ç ç»“æ„
            entities = self._parse_file_content(content, lang, relative_path)
            
            # å­˜å‚¨æ–‡ä»¶ç»“æ„
            self.file_structure[relative_path] = {
                'language': lang,
                'size': file_path.stat().st_size,
                'lines': stats['total_lines'],
                'entities': entities,
                'stats': stats,
            }
            
            # å­˜å‚¨ä»£ç å®ä½“
            if lang not in self.code_entities:
                self.code_entities[lang] = []
            self.code_entities[lang].extend(entities)
            
        except Exception as e:
            print(f"âš ï¸  åˆ†ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    def _count_file_stats(self, content: str) -> Dict[str, int]:
        """ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°"""
        lines = content.split('\n')
        total_lines = len(lines)
        
        code_lines = 0
        comment_lines = 0
        blank_lines = 0
        
        in_block_comment = False
        
        for line in lines:
            stripped = line.strip()
            
            if not stripped:
                blank_lines += 1
            elif stripped.startswith('//') or stripped.startswith('#') or stripped.startswith('--'):
                comment_lines += 1
            elif stripped.startswith('/*'):
                comment_lines += 1
                if '*/' not in stripped:
                    in_block_comment = True
            elif '*/' in stripped:
                comment_lines += 1
                in_block_comment = False
            elif in_block_comment:
                comment_lines += 1
            else:
                code_lines += 1
        
        return {
            'total_lines': total_lines,
            'code_lines': code_lines,
            'comment_lines': comment_lines,
            'blank_lines': blank_lines,
        }
    
    def _parse_file_content(self, content: str, lang: str, file_path: str) -> List[Dict]:
        """è§£ææ–‡ä»¶å†…å®¹ï¼Œæå–ä»£ç å®ä½“"""
        entities = []
        
        if lang == 'python':
            entities = self._parse_python(content, file_path)
        elif lang in ['javascript', 'typescript']:
            entities = self._parse_javascript(content, file_path, lang)
        elif lang == 'java':
            entities = self._parse_java(content, file_path)
        elif lang == 'go':
            entities = self._parse_go(content, file_path)
        elif lang == 'rust':
            entities = self._parse_rust(content, file_path)
        else:
            # å¯¹äºä¸æ”¯æŒè¯¦ç»†è§£æçš„è¯­è¨€ï¼Œè‡³å°‘æå–å‡½æ•°å’Œç±»çš„åŸºæœ¬ä¿¡æ¯
            entities = self._parse_generic(content, file_path, lang)
        
        return entities
    
    def _parse_python(self, content: str, file_path: str) -> List[Dict]:
        """è§£æPythonä»£ç """
        entities = []
        lines = content.split('\n')
        
        # ç®€å•çš„Pythonè§£æ
        class_pattern = re.compile(r'^class\s+(\w+)')
        function_pattern = re.compile(r'^def\s+(\w+)')
        async_function_pattern = re.compile(r'^async\s+def\s+(\w+)')
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # æ£€æŸ¥ç±»å®šä¹‰
            class_match = class_pattern.match(line_stripped)
            if class_match:
                entities.append({
                    'type': 'class',
                    'name': class_match.group(1),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'python',
                })
            
            # æ£€æŸ¥å‡½æ•°å®šä¹‰
            func_match = function_pattern.match(line_stripped)
            async_func_match = async_function_pattern.match(line_stripped)
            
            if func_match:
                entities.append({
                    'type': 'function',
                    'name': func_match.group(1),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'python',
                    'async': False,
                })
            elif async_func_match:
                entities.append({
                    'type': 'function',
                    'name': async_func_match.group(1),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'python',
                    'async': True,
                })
        
        return entities
    
    def _parse_javascript(self, content: str, file_path: str, lang: str) -> List[Dict]:
        """è§£æJavaScript/TypeScriptä»£ç """
        entities = []
        lines = content.split('\n')
        
        # JavaScript/TypeScriptæ¨¡å¼
        class_pattern = re.compile(r'^(export\s+)?(abstract\s+)?class\s+(\w+)')
        function_pattern = re.compile(r'^(export\s+)?(async\s+)?function\s+(\w+)')
        arrow_function_pattern = re.compile(r'^(export\s+)?(const|let|var)\s+(\w+)\s*=\s*(async\s*)?\([^)]*\)\s*=>')
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # æ£€æŸ¥ç±»å®šä¹‰
            class_match = class_pattern.match(line_stripped)
            if class_match:
                entities.append({
                    'type': 'class',
                    'name': class_match.group(3),
                    'line': i + 1,
                    'file': file_path,
                    'language': lang,
                    'exported': bool(class_match.group(1)),
                })
            
            # æ£€æŸ¥å‡½æ•°å®šä¹‰
            func_match = function_pattern.match(line_stripped)
            if func_match:
                entities.append({
                    'type': 'function',
                    'name': func_match.group(3),
                    'line': i + 1,
                    'file': file_path,
                    'language': lang,
                    'async': bool(func_match.group(2)),
                    'exported': bool(func_match.group(1)),
                })
            
            # æ£€æŸ¥ç®­å¤´å‡½æ•°
            arrow_match = arrow_function_pattern.match(line_stripped)
            if arrow_match:
                entities.append({
                    'type': 'function',
                    'name': arrow_match.group(3),
                    'line': i + 1,
                    'file': file_path,
                    'language': lang,
                    'async': bool(arrow_match.group(4)),
                    'exported': bool(arrow_match.group(1)),
                    'arrow': True,
                })
        
        return entities
    
    def _parse_java(self, content: str, file_path: str) -> List[Dict]:
        """è§£æJavaä»£ç """
        entities = []
        lines = content.split('\n')
        
        # Javaæ¨¡å¼
        class_pattern = re.compile(r'^(public|private|protected|static|final|abstract|sealed|non-sealed)?\s*(class|interface|enum|record)\s+(\w+)')
        method_pattern = re.compile(r'^(public|private|protected|static|final|abstract|synchronized|native|strictfp)?\s*(\w+\s+)*(\w+)\s*\([^)]*\)\s*{')
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # æ£€æŸ¥ç±»/æ¥å£/æšä¸¾å®šä¹‰
            class_match = class_pattern.match(line_stripped)
            if class_match:
                entities.append({
                    'type': class_match.group(2),  # class, interface, enum, record
                    'name': class_match.group(3),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'java',
                    'modifiers': class_match.group(1).split() if class_match.group(1) else [],
                })
            
            # æ£€æŸ¥æ–¹æ³•å®šä¹‰
            method_match = method_pattern.match(line_stripped)
            if method_match and 'class' not in line_stripped and 'interface' not in line_stripped:
                entities.append({
                    'type': 'method',
                    'name': method_match.group(3),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'java',
                    'modifiers': method_match.group(1).split() if method_match.group(1) else [],
                })
        
        return entities
    
    def _parse_go(self, content: str, file_path: str) -> List[Dict]:
        """è§£æGoä»£ç """
        entities = []
        lines = content.split('\n')
        
        # Goæ¨¡å¼
        func_pattern = re.compile(r'^func\s+(\(\s*\*\s*\w+\s*\)\s*)?(\w+)\s*\([^)]*\)')
        struct_pattern = re.compile(r'^type\s+(\w+)\s+struct')
        interface_pattern = re.compile(r'^type\s+(\w+)\s+interface')
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # æ£€æŸ¥å‡½æ•°å®šä¹‰
            func_match = func_pattern.match(line_stripped)
            if func_match:
                entities.append({
                    'type': 'function',
                    'name': func_match.group(2),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'go',
                    'receiver': func_match.group(1) if func_match.group(1) else None,
                })
            
            # æ£€æŸ¥ç»“æ„ä½“å®šä¹‰
            struct_match = struct_pattern.match(line_stripped)
            if struct_match:
                entities.append({
                    'type': 'struct',
                    'name': struct_match.group(1),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'go',
                })
            
            # æ£€æŸ¥æ¥å£å®šä¹‰
            interface_match = interface_pattern.match(line_stripped)
            if interface_match:
                entities.append({
                    'type': 'interface',
                    'name': interface_match.group(1),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'go',
                })
        
        return entities
    
    def _parse_rust(self, content: str, file_path: str) -> List[Dict]:
        """è§£æRustä»£ç """
        entities = []
        lines = content.split('\n')
        
        # Rustæ¨¡å¼
        struct_pattern = re.compile(r'^(pub\s+)?struct\s+(\w+)')
        enum_pattern = re.compile(r'^(pub\s+)?enum\s+(\w+)')
        trait_pattern = re.compile(r'^(pub\s+)?trait\s+(\w+)')
        function_pattern = re.compile(r'^(pub\s+)?fn\s+(\w+)\s*\([^)]*\)')
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # æ£€æŸ¥ç»“æ„ä½“å®šä¹‰
            struct_match = struct_pattern.match(line_stripped)
            if struct_match:
                entities.append({
                    'type': 'struct',
                    'name': struct_match.group(2),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'rust',
                    'pub': bool(struct_match.group(1)),
                })
            
            # æ£€æŸ¥æšä¸¾å®šä¹‰
            enum_match = enum_pattern.match(line_stripped)
            if enum_match:
                entities.append({
                    'type': 'enum',
                    'name': enum_match.group(2),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'rust',
                    'pub': bool(enum_match.group(1)),
                })
            
            # æ£€æŸ¥traitå®šä¹‰
            trait_match = trait_pattern.match(line_stripped)
            if trait_match:
                entities.append({
                    'type': 'trait',
                    'name': trait_match.group(2),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'rust',
                    'pub': bool(trait_match.group(1)),
                })
            
            # æ£€æŸ¥å‡½æ•°å®šä¹‰
            func_match = function_pattern.match(line_stripped)
            if func_match:
                entities.append({
                    'type': 'function',
                    'name': func_match.group(2),
                    'line': i + 1,
                    'file': file_path,
                    'language': 'rust',
                    'pub': bool(func_match.group(1)),
                })
        
        return entities
    
    def _parse_generic(self, content: str, file_path: str, lang: str) -> List[Dict]:
        """é€šç”¨è§£æå™¨ï¼Œç”¨äºä¸æ”¯æŒè¯¦ç»†è§£æçš„è¯­è¨€"""
        entities = []
        lines = content.split('\n')
        
        # é€šç”¨æ¨¡å¼ï¼šæŸ¥æ‰¾çœ‹èµ·æ¥åƒå‡½æ•°æˆ–ç±»å®šä¹‰çš„è¡Œ
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # ç®€å•çš„å¯å‘å¼è§„åˆ™
            if lang in ['c', 'cpp', 'csharp']:
                # Cå®¶æ—è¯­è¨€ï¼šæŸ¥æ‰¾ä»¥ç±»å‹å¼€å¤´çš„å‡½æ•°å®šä¹‰
                if re.match(r'^\w+\s+\w+\s*\([^)]*\)\s*{', line_stripped):
                    # æå–å‡½æ•°å
                    match = re.match(r'^\w+\s+(\w+)\s*\([^)]*\)', line_stripped)
                    if match:
                        entities.append({
                            'type': 'function',
                            'name': match.group(1),
                            'line': i + 1,
                            'file': file_path,
                            'language': lang,
                        })
            
            elif lang == 'ruby':
                # Rubyï¼šæŸ¥æ‰¾defå¼€å¤´çš„è¡Œ
                if line_stripped.startswith('def '):
                    name = line_stripped[4:].split('(')[0].strip()
                    entities.append({
                        'type': 'method',
                        'name': name,
                        'line': i + 1,
                        'file': file_path,
                        'language': lang,
                    })
            
            elif lang == 'php':
                # PHPï¼šæŸ¥æ‰¾functionå¼€å¤´çš„è¡Œ
                if line_stripped.startswith('function '):
                    name = line_stripped[9:].split('(')[0].strip()
                    entities.append({
                        'type': 'function',
                        'name': name,
                        'line': i + 1,
                        'file': file_path,
                        'language': lang,
                    })
        
        return entities
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        # è¿‡æ»¤æœ‰æ–‡ä»¶çš„è¯­è¨€
        active_languages = {lang: stats for lang, stats in self.language_stats.items() if stats['file_count'] > 0}
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_files = sum(stats['file_count'] for stats in active_languages.values())
        total_lines = sum(stats['total_lines'] for stats in active_languages.values())
        
        return {
            'project_path': str(self.project_path),
            'total_files': total_files,
            'total_lines': total_lines,
            'languages': active_languages,
            'file_count': len(self.file_structure),
            'entity_count': sum(len(entities) for entities in self.code_entities.values()),
        }
    
    def generate_project_map(self) -> str:
        """ç”Ÿæˆé¡¹ç›®æ˜ å°„çš„Markdownæ–‡æ¡£"""
        lines = []
        
        lines.append("# å¤šè¯­è¨€é¡¹ç›®ç»“æ„æ˜ å°„")
        lines.append("")
        lines.append(f"é¡¹ç›®è·¯å¾„: `{self.project_path}`")
        lines.append(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"æ–‡ä»¶æ€»æ•°: {len(self.file_structure)}")
        lines.append(f"ä»£ç å®ä½“æ€»æ•°: {sum(len(entities) for entities in self.code_entities.values())}")
        lines.append("")
        
        # è¯­è¨€ç»Ÿè®¡
        lines.append("## è¯­è¨€ç»Ÿè®¡")
        lines.append("")
        lines.append("| è¯­è¨€ | æ–‡ä»¶æ•° | æ€»è¡Œæ•° | ä»£ç è¡Œ | æ³¨é‡Šè¡Œ | ç©ºè¡Œ |")
        lines.append("|------|--------|--------|--------|--------|------|")
        
        for lang, stats in sorted(self.language_stats.items()):
            if stats['file_count'] > 0:
                lines.append(f"| {lang} | {stats['file_count']} | {stats['total_lines']} | {stats['code_lines']} | {stats['comment_lines']} | {stats['blank_lines']} |")
        
        lines.append("")
        
        # æŒ‰è¯­è¨€æ˜¾ç¤ºä»£ç å®ä½“
        for lang, entities in sorted(self.code_entities.items()):
            if entities:
                lines.append(f"## {lang.capitalize()} ä»£ç å®ä½“")
                lines.append("")
                
                # æŒ‰æ–‡ä»¶åˆ†ç»„
                files = {}
                for entity in entities:
                    file_path = entity['file']
                    if file_path not in files:
                        files[file_path] = []
                    files[file_path].append(entity)
                
                for file_path, file_entities in sorted(files.items()):
                    lines.append(f"### æ–‡ä»¶: `{file_path}`")
                    lines.append("")
                    
                    for entity in sorted(file_entities, key=lambda x: x['line']):
                        entity_type = entity.get('type', 'unknown')
                        entity_name = entity.get('name', 'unknown')
                        line_num = entity.get('line', 0)
                        
                        lines.append(f"- **ç¬¬ {line_num} è¡Œ**: `{entity_type}` `{entity_name}`")
                        
                        # æ·»åŠ é¢å¤–ä¿¡æ¯
                        extra_info = []
                        if entity.get('async'):
                            extra_info.append('async')
                        if entity.get('exported'):
                            extra_info.append('exported')
                        if entity.get('pub'):
                            extra_info.append('pub')
                        if entity.get('modifiers'):
                            extra_info.extend(entity['modifiers'])
                        
                        if extra_info:
                            lines.append(f"  - ä¿®é¥°ç¬¦: {', '.join(extra_info)}")
                
                lines.append("")
        
        # æ–‡ä»¶ç»“æ„æ¦‚è§ˆ
        lines.append("## æ–‡ä»¶ç»“æ„æ¦‚è§ˆ")
        lines.append("")
        
        for file_path, file_info in sorted(self.file_structure.items()):
            lang = file_info['language']
            size_kb = file_info['size'] / 1024
            lines_count = file_info['lines']
            entity_count = len(file_info['entities'])
            
            lines.append(f"- `{file_path}`")
            lines.append(f"  - è¯­è¨€: {lang}")
            lines.append(f"  - å¤§å°: {size_kb:.1f} KB")
            lines.append(f"  - è¡Œæ•°: {lines_count}")
            lines.append(f"  - å®ä½“æ•°: {entity_count}")
            
            # æ˜¾ç¤ºä¸»è¦å®ä½“
            if entity_count > 0:
                main_entities = file_info['entities'][:3]  # åªæ˜¾ç¤ºå‰3ä¸ª
                entity_names = [f"`{e['name']}`" for e in main_entities]
                lines.append(f"  - ä¸»è¦å®ä½“: {', '.join(entity_names)}")
                if entity_count > 3:
                    lines.append(f"  - ... è¿˜æœ‰ {entity_count - 3} ä¸ªå®ä½“")
        
        return '\n'.join(lines)
    
    def save_project_map(self, output_file: str = "project_structure_map.md") -> Path:
        """ä¿å­˜é¡¹ç›®æ˜ å°„åˆ°æ–‡ä»¶"""
        content = self.generate_project_map()
        output_path = self.project_path / output_file
        output_path.write_text(content, encoding='utf-8')
        print(f"ğŸ“ é¡¹ç›®ç»“æ„æ˜ å°„å·²ä¿å­˜åˆ°: {output_path}")
        return output_path


# ============================================================================
# å¢å¼ºç‰ˆç±»æ–¹æ³•æ˜ å°„å™¨ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
# ============================================================================

class EnhancedClassMethodMapper:
    """å¢å¼ºç‰ˆç±»æ–¹æ³•æ˜ å°„å™¨ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
    
    def __init__(self, project_path: Path):
        self.project_path = project_path
        self.python_mapper = ClassMethodMapper(project_path)
        self.multi_lang_analyzer = MultiLangAnalyzer(project_path)
        
    def analyze_project(self) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªé¡¹ç›®ï¼ˆå¤šè¯­è¨€ï¼‰"""
        print(f"ğŸ” å¼€å§‹å¢å¼ºç‰ˆé¡¹ç›®ç»“æ„åˆ†æ: {self.project_path}")
        
        # åˆ†æPythonä»£ç ï¼ˆè¯¦ç»†åˆ†æï¼‰
        python_summary = self.python_mapper.analyze_project()
        
        # åˆ†æå¤šè¯­è¨€ä»£ç ï¼ˆåŸºç¡€åˆ†æï¼‰
        multi_lang_summary = self.multi_lang_analyzer.analyze_project()
        
        # åˆå¹¶ç»“æœ
        combined_summary = {
            'project_path': str(self.project_path),
            'python_analysis': python_summary,
            'multi_lang_analysis': multi_lang_summary,
            'total_files': multi_lang_summary['total_files'],
            'total_lines': multi_lang_summary['total_lines'],
            'language_count': len(multi_lang_summary['languages']),
        }
        
        return combined_summary
    
    def generate_enhanced_map(self) -> str:
        """ç”Ÿæˆå¢å¼ºç‰ˆé¡¹ç›®æ˜ å°„"""
        # è·å–Pythonè¯¦ç»†æ˜ å°„
        python_map = self.python_mapper.generate_class_method_map()
        
        # è·å–å¤šè¯­è¨€æ˜ å°„
        multi_lang_map = self.multi_lang_analyzer.generate_project_map()
        
        # åˆå¹¶æ˜ å°„
        lines = []
        lines.append("# é¡¹ç›®ç»“æ„æ˜ å°„")
        lines.append("")
        lines.append("> æœ¬æ–‡æ¡£åŒ…å«é¡¹ç›®çš„ç±»ã€æ–¹æ³•ã€å‡½æ•°ç­‰ä»£ç ç»“æ„ä¿¡æ¯")
        lines.append("")
        lines.append(f"é¡¹ç›®è·¯å¾„: `{self.project_path}`")
        lines.append("")
        
        # æ·»åŠ å¤šè¯­è¨€æ¦‚è§ˆ
        lines.append("## å¤šè¯­è¨€é¡¹ç›®æ¦‚è§ˆ")
        lines.append("")
        lines.append(multi_lang_map.split("## è¯­è¨€ç»Ÿè®¡")[1].split("##")[0].strip())
        lines.append("")
        
        # æ·»åŠ Pythonè¯¦ç»†åˆ†æ
        lines.append("## Pythonä»£ç è¯¦ç»†åˆ†æ")
        lines.append("")
        # è·³è¿‡Pythonæ˜ å°„çš„æ ‡é¢˜éƒ¨åˆ†
        python_content = python_map.split("# é¡¹ç›®ç±»æ–¹æ³•æ˜ å°„")[1]
        lines.append(python_content)
        
        return '\n'.join(lines)
    
    def save_enhanced_map(self, output_file: str = "project_structure.md") -> Path:
        """ä¿å­˜å¢å¼ºç‰ˆæ˜ å°„åˆ°æ–‡ä»¶ï¼ˆé¡¹ç›®ç»“æ„ï¼šç±»ã€æ–¹æ³•ã€å‡½æ•°ç­‰ï¼‰"""
        content = self.generate_enhanced_map()
        output_path = self.project_path / output_file
        output_path.write_text(content, encoding='utf-8')
        print(f"ğŸ“ é¡¹ç›®ç»“æ„æ˜ å°„å·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def get_language_summary(self) -> str:
        """è·å–è¯­è¨€æ‘˜è¦ï¼ˆç”¨äºAgentæç¤ºï¼‰"""
        multi_lang_summary = self.multi_lang_analyzer._generate_summary()
        
        lines = []
        lines.append("## é¡¹ç›®è¯­è¨€æ„æˆ")
        lines.append("")
        
        if not multi_lang_summary['languages']:
            lines.append("é¡¹ç›®ç›®å½•ä¸ºç©ºæˆ–æ²¡æœ‰å¯åˆ†æçš„ä»£ç æ–‡ä»¶ã€‚")
            return '\n'.join(lines)
        
        lines.append("æ£€æµ‹åˆ°çš„ç¼–ç¨‹è¯­è¨€:")
        for lang, stats in multi_lang_summary['languages'].items():
            lines.append(f"- **{lang}**: {stats['file_count']} ä¸ªæ–‡ä»¶, {stats['total_lines']} è¡Œä»£ç ")
        
        lines.append("")
        lines.append("## ä¸»è¦ä»£ç å®ä½“")
        lines.append("")
        
        # æ˜¾ç¤ºä¸»è¦è¯­è¨€çš„ä»£ç å®ä½“
        for lang, entities in self.multi_lang_analyzer.code_entities.items():
            if entities:
                entity_count = len(entities)
                # æŒ‰ç±»å‹åˆ†ç»„
                type_counts = {}
                for entity in entities:
                    entity_type = entity.get('type', 'unknown')
                    type_counts[entity_type] = type_counts.get(entity_type, 0) + 1
                
                lines.append(f"**{lang.capitalize()}**: {entity_count} ä¸ªä»£ç å®ä½“")
                for entity_type, count in type_counts.items():
                    lines.append(f"  - {entity_type}: {count} ä¸ª")
        
        return '\n'.join(lines)
    
    def update_analysis(self, changed_files: Optional[List[Path]] = None) -> bool:
        """æ›´æ–°åˆ†æç»“æœ"""
        try:
            # æ›´æ–°Pythonåˆ†æ
            self.python_mapper.update_class_method_map(changed_files)
            
            # å¯¹äºå¤šè¯­è¨€åˆ†æï¼Œæš‚æ—¶é‡æ–°åˆ†ææ•´ä¸ªé¡¹ç›®
            # æœªæ¥å¯ä»¥ä¼˜åŒ–ä¸ºå¢é‡æ›´æ–°
            self.multi_lang_analyzer.analyze_project()
            
            return True
        except Exception as e:
            print(f"âš ï¸  æ›´æ–°åˆ†æå¤±è´¥: {e}")
            return False
    
    # å…¼å®¹æ€§æ–¹æ³•
    def update_class_method_map(self, changed_files: Optional[List[Path]] = None) -> bool:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šæ›´æ–°ç±»æ–¹æ³•æ˜ å°„ï¼ˆè°ƒç”¨update_analysisï¼‰"""
        return self.update_analysis(changed_files)
    
    def save_class_method_map(self, output_file: str = "class_method_map.md") -> Path:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šä¿å­˜ç±»æ–¹æ³•æ˜ å°„ï¼ˆè°ƒç”¨åŸºç¡€ç‰ˆï¼‰"""
        return self.python_mapper.save_class_method_map(output_file)


def analyze_enhanced_project(project_path: str) -> Path:
    """åˆ†æå¢å¼ºç‰ˆé¡¹ç›®å¹¶ç”Ÿæˆæ˜ å°„çš„ä¾¿æ·å‡½æ•°"""
    mapper = EnhancedClassMethodMapper(Path(project_path))
    mapper.analyze_project()
    return mapper.save_enhanced_map()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys
    if len(sys.argv) > 1:
        project_path = sys.argv[1]
    else:
        project_path = "."
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    mapper = ClassMethodMapper(Path(project_path))
    mapper.analyze_project()
    output_file = mapper.save_class_method_map()
    print(f"âœ… åŸºæœ¬åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
    
    # æµ‹è¯•å¢å¼ºåŠŸèƒ½
    enhanced_mapper = EnhancedClassMethodMapper(Path(project_path))
    enhanced_mapper.analyze_project()
    enhanced_output = enhanced_mapper.save_enhanced_map()
    print(f"âœ… å¢å¼ºåˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {enhanced_output}")
    
    # æ˜¾ç¤ºè¯­è¨€æ‘˜è¦
    summary = enhanced_mapper.get_language_summary()
    print(f"\nğŸ“Š è¯­è¨€æ‘˜è¦:")
    print(summary)